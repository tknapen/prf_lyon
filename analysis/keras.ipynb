{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pRF fitting using a Keras CNN\n",
    "\n",
    "this notebook implements pRF fitting using a CNN defined in keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/knapen/software/miniconda3/envs/py36/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, json\n",
    "import nibabel as nb\n",
    "\n",
    "from keras import metrics \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LocallyConnected1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "base_dir = '/home/knapen/projects/prf_lyon/'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "os.chdir(os.path.join(base_dir, 'analysis'))\n",
    "from prf_fit import *\n",
    "\n",
    "with open(os.path.join(base_dir, 'analysis', 'settings.json')) as f:\n",
    "    json_s = f.read()\n",
    "    analysis_info = json.loads(json_s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we have to load some data to prepare things.\n",
    "Most of this is taken from an earlier notebook that implements the grid search stage of pRF fitting, much of it is likely to be redundant. It creates a prf fit object that can load or create grid search regressors. For now, we're only using its functionality to load a design matrix.\n",
    "\n",
    "Read on for the good stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "sub = 'sub-01'\n",
    "\n",
    "input_file = os.path.join(base_dir, 'data', sub, 'func', \\\n",
    "    sub+'_task-prf_acq-median_T1w_desc-preproc_bold.nii.gz')\n",
    "dm_file = os.path.join(base_dir, 'data', sub, 'dm_out.npy')\n",
    "    \n",
    "mask_file = nb.load(os.path.join(base_dir, 'data', sub, 'func', \\\n",
    "     sub+'_task-prf_dir-AP_run-1_space-T1w_desc-brain_mask.nii.gz'))\n",
    "mask = mask_file.get_data().astype(bool)\n",
    "\n",
    "# for registration into pycortex\n",
    "example_epi_file = os.path.join(base_dir, 'data', sub, 'func', \\\n",
    "     sub+'_task-prf_dir-AP_run-1_space-T1w_boldref.nii.gz')\n",
    "T1_file = os.path.join(base_dir, 'data', sub, 'anat', \\\n",
    "     sub+'_desc-preproc_T1w.nii.gz')\n",
    "fs_T1_file = os.path.join(base_dir, 'data', sub, 'anat', \\\n",
    "     'T1.nii.gz')\n",
    "\n",
    "#design matrix\n",
    "visual_dm = np.load(dm_file).T\n",
    "\n",
    "# data\n",
    "in_file_nii = nb.load(input_file)\n",
    "data = in_file_nii.get_data().reshape((-1,in_file_nii.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "fit_model = analysis_info[\"fit_model\"]\n",
    "N_PROCS = 8\n",
    "\n",
    "# Fit: define search grids\n",
    "x_grid_bound = (-analysis_info[\"max_eccen\"], analysis_info[\"max_eccen\"])\n",
    "y_grid_bound = (-analysis_info[\"max_eccen\"], analysis_info[\"max_eccen\"])\n",
    "sigma_grid_bound = (analysis_info[\"min_size\"], analysis_info[\"max_size\"])\n",
    "n_grid_bound = (analysis_info[\"min_n\"], analysis_info[\"max_n\"])\n",
    "grid_steps = analysis_info[\"grid_steps\"]\n",
    "\n",
    "# Fit: define search bounds\n",
    "x_fit_bound = (-analysis_info[\"max_eccen\"]*2, analysis_info[\"max_eccen\"]*2)\n",
    "y_fit_bound = (-analysis_info[\"max_eccen\"]*2, analysis_info[\"max_eccen\"]*2)\n",
    "sigma_fit_bound = (1e-6, 1e2)\n",
    "n_fit_bound = (1e-6, 2)\n",
    "beta_fit_bound = (-1e6, 1e6)\n",
    "baseline_fit_bound = (-1e6, 1e6)\n",
    "\n",
    "if fit_model == 'gauss' or fit_model == 'gauss_sg':\n",
    "    bound_grids  = (x_grid_bound, y_grid_bound, sigma_grid_bound)\n",
    "    bound_fits = (x_fit_bound, y_fit_bound, sigma_fit_bound, beta_fit_bound, baseline_fit_bound)\n",
    "elif fit_model == 'css' or fit_model == 'css_sg':\n",
    "    bound_grids  = (x_grid_bound, y_grid_bound, sigma_grid_bound, n_grid_bound)\n",
    "    bound_fits = (x_fit_bound, y_fit_bound, sigma_fit_bound, n_fit_bound, beta_fit_bound, baseline_fit_bound)\n",
    "\n",
    "# intitialize prf analysis\n",
    "prf = PRF_fit(  data = data[mask.ravel()],\n",
    "                fit_model = fit_model, \n",
    "                visual_design = visual_dm, \n",
    "                screen_distance = analysis_info[\"screen_distance\"],\n",
    "                screen_width = analysis_info[\"screen_width\"],\n",
    "                scale_factor = 1/2.0, \n",
    "                tr =  analysis_info[\"TR\"],\n",
    "                bound_grids = bound_grids,\n",
    "                grid_steps = grid_steps,\n",
    "                bound_fits = bound_fits,\n",
    "                n_jobs = N_PROCS,\n",
    "                sg_filter_window_length = analysis_info[\"sg_filt_window_length\"],\n",
    "                sg_filter_polyorder = analysis_info[\"sg_filt_polyorder\"],\n",
    "                sg_filter_deriv = analysis_info[\"sg_filt_deriv\"], \n",
    "                )\n",
    "# will need to move/delete this file for new predictions\n",
    "prediction_file = os.path.join(base_dir, 'data', 'sub-01', 'predictions.npy')\n",
    "if os.path.isfile(prediction_file):\n",
    "    prf.load_grid_predictions(prediction_file=prediction_file)\n",
    "else:\n",
    "    prf.make_predictions(out_file=prediction_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct training set.\n",
    "From the grid search stage, we have a set of N (8000, in some version) regressors that span the grid for initial search. We also need to add intercept and slope parameters, so we add random intercepts and slopes (betas), multiplying the amount of training regressors. Also, because the network should be trained on far more than this, we create an even larger training set by \"blowing up\" this initial set of grid regressors. What we do, is just add noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "blow_up = 100\n",
    "# make extra predictions, with added noise\n",
    "blow_up_noisy_predictions = np.repeat(prf.predictions, blow_up, axis=1)\n",
    "# scale predictions randomly\n",
    "betas = np.random.rand(blow_up_noisy_predictions.shape[1])+0.25\n",
    "blow_up_noisy_predictions *= betas\n",
    "# random offsets\n",
    "offsets = np.random.randn(blow_up_noisy_predictions.shape[1])\n",
    "blow_up_noisy_predictions += offsets\n",
    "\n",
    "blow_up_noisy_predictions += np.random.randn(prf.predictions.shape[0], prf.predictions.shape[1] * blow_up)/5.0\n",
    "\n",
    "extra_parameters = np.array([betas, offsets])\n",
    "\n",
    "n_prf_parameters = 5\n",
    "parameters = np.array([prf.prf_xs, prf.prf_ys, prf.prf_sigma]).reshape((3,-1))\n",
    "\n",
    "blow_up_parameters = np.vstack((np.repeat(parameters, blow_up, axis=1), extra_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup and training\n",
    "Now, we're ready to specify the model architecture. This is a simple 1D model with a very limited nr of layers. It should take the data timecourse and predict the 5 regressors. So, it starts with a convolutional set of layers `Conv1D`, and ends with a set of `Dense` layers that are linear (not softmaxed or anything). In the middle I have put a set of `LocallyConnected1D` - otherwise the network is just very poor :). The metrics and loss are specified also in line with this regression nature of the problem.\n",
    "\n",
    "This architecture is not at all optimized for what it's supposed to do, but the first try already worked. We should discuss the possible architectural options in future meetings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=prf.n_timepoints//2, kernel_size=4, input_shape=(prf.n_timepoints,1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=prf.n_timepoints//4, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LocallyConnected1D(filters=prf.n_timepoints//8, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LocallyConnected1D(filters=prf.n_timepoints//16, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "# model.add(Conv1D(filters=prf.n_timepoints//32, kernel_size=16, activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_prf_parameters*2, kernel_initializer='uniform'))\n",
    "model.add(Dense(n_prf_parameters))\n",
    "\n",
    "print(model.summary())\n",
    "plot_model(model, show_shapes=True, to_file='keras_1.pdf')\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='nadam',\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "Next, we fit (train) the model. With the GPU in aeneas, this epoch and batch size is faster than anything else I've used, but this can still be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd5790ab5c0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(prf.predictions.T[:,:,np.newaxis], parameters.T, epochs=10, batch_size=32, verbose=3)\n",
    "model.fit(blow_up_noisy_predictions.T[:,:,np.newaxis], blow_up_parameters.T, \n",
    "          epochs=10, \n",
    "          batch_size=256, \n",
    "          verbose=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the pRFs \n",
    "Next, we use the trained model to fit the pRF parameters on a whole-brain dataset of ~250k voxels, and time it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 551 ms, total: 13.2 s\n",
      "Wall time: 9.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_pars = model.predict(prf.data[:,:,np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks \n",
    "Just checking whether this model recoups the parameters, here quantified by means of just correlations like a loser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.99833249407842617, 0.0)\n"
     ]
    }
   ],
   "source": [
    "parameters = np.array([prf.prf_xs, prf.prf_ys, prf.prf_sigma, np.ones_like(prf.prf_sigma), np.zeros_like(prf.prf_sigma) ]).reshape((5,-1))\n",
    "pred = model.predict(prf.predictions.T[:,:,np.newaxis])\n",
    "\n",
    "print(sp.stats.pearsonr(pred.ravel(), parameters.T[:].ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "py36"
  },
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
